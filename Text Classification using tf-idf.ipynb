{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\IRPHAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\IRPHAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\IRPHAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\IRPHAN\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import psycopg2\r\n",
    "#for text pre-processing\r\n",
    "import re, string\r\n",
    "import nltk\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from nltk.tokenize import word_tokenize\r\n",
    "from nltk.stem import SnowballStemmer\r\n",
    "from nltk.corpus import wordnet\r\n",
    "from nltk.stem import WordNetLemmatizer\r\n",
    "nltk.download('punkt')\r\n",
    "nltk.download('averaged_perceptron_tagger')\r\n",
    "nltk.download('wordnet')\r\n",
    "#for model-building\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\r\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\r\n",
    "# bag of words\r\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "#for word embedding\r\n",
    "import gensim\r\n",
    "from gensim.models import Word2Vec\r\n",
    "from typing import List\r\n",
    "from jpype import JClass, JString, getDefaultJVMPath, shutdownJVM, startJVM, java\r\n",
    "\r\n",
    "ZEMBEREK_PATH = r'C:\\Users\\IRPHAN\\Desktop\\zemberek-full.jar'\r\n",
    "startJVM(getDefaultJVMPath(), '-ea', '-Djava.class.path=%s' % (ZEMBEREK_PATH))\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = psycopg2.connect(user = \"postgres\",\n",
    "                                  password = \"42088\",\n",
    "                                  host=\"127.0.0.1\",\n",
    "                                  port=\"5432\",\n",
    "                                  database=\"postgres\")                           \n",
    "cursor = connection.cursor()\n",
    "cursor2 = connection.cursor()\n",
    "cursor3 = connection.cursor()\n",
    "postgreSQL_select_Query = \"select * from public.train\" #our trained data\n",
    "postgreSQL_select_Query2 = \"select * from public.data_test\" #our test data\n",
    "postgreSQL_select_Query3 = \"select * from public.trained_data\" #our comp data\n",
    "cursor.execute(postgreSQL_select_Query)\n",
    "cursor2.execute(postgreSQL_select_Query2)   \n",
    "cursor3.execute(postgreSQL_select_Query3)   \n",
    "records = cursor.fetchall()\n",
    "records2 = cursor2.fetchall()\n",
    "records3 = cursor3.fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comp = pd.DataFrame(records3, columns=[\"id\", \"metin\",\"network_ici\",\n",
    "                    \"network_disi\",\n",
    "                    \"genisbant\",\n",
    "                    \"gb_kopma\",\n",
    "                    \"gb_altyapi\",\n",
    "                    \"gb_port\",\n",
    "                    \"gb_kablo\",\n",
    "                    \"gb_hizveping\",\n",
    "                    \"gb_modem\",\n",
    "                    \"sabithat\",\n",
    "                    \"sh_kopma\",\n",
    "                    \"sh_altyapi\",\n",
    "                    \"mobil\",\n",
    "                    \"mb_baglanti\",\n",
    "                    \"mb_hizveveri\",\n",
    "                    \"mb_kopma\",\n",
    "                    \"tivibu\",\n",
    "                    \"tv_cihaz\",\n",
    "                    \"tv_yayin\",\n",
    "                    \"siparis_nakil\",\n",
    "                    \"ekip\",\n",
    "                    \"bayi\",\n",
    "                    \"diger\"])\n",
    "df_train = pd.DataFrame(records, columns=[\"id\", \"metin\",\"network_ici\",\n",
    "                    \"network_disi\",\n",
    "                    \"genisbant\",\n",
    "                    \"gb_kopma\",\n",
    "                    \"gb_altyapi\",\n",
    "                    \"gb_port\",\n",
    "                    \"gb_kablo\",\n",
    "                    \"gb_hizveping\",\n",
    "                    \"gb_modem\",\n",
    "                    \"sabithat\",\n",
    "                    \"sh_kopma\",\n",
    "                    \"sh_altyapi\",\n",
    "                    \"mobil\",\n",
    "                    \"mb_baglanti\",\n",
    "                    \"mb_hizveveri\",\n",
    "                    \"mb_kopma\",\n",
    "                    \"tivibu\",\n",
    "                    \"tv_cihaz\",\n",
    "                    \"tv_yayin\",\n",
    "                    \"siparis_nakil\",\n",
    "                    \"ekip\",\n",
    "                    \"bayi\",\n",
    "                    \"diger\"])\n",
    "df_test = pd.DataFrame(records2, columns=[\"id\", \"metin\",\"network_ici\",\n",
    "                    \"network_disi\",\n",
    "                    \"genisbant\",\n",
    "                    \"gb_kopma\",\n",
    "                    \"gb_altyapi\",\n",
    "                    \"gb_port\",\n",
    "                    \"gb_kablo\",\n",
    "                    \"gb_hizveping\",\n",
    "                    \"gb_modem\",\n",
    "                    \"sabithat\",\n",
    "                    \"sh_kopma\",\n",
    "                    \"sh_altyapi\",\n",
    "                    \"mobil\",\n",
    "                    \"mb_baglanti\",\n",
    "                    \"mb_hizveveri\",\n",
    "                    \"mb_kopma\",\n",
    "                    \"tivibu\",\n",
    "                    \"tv_cihaz\",\n",
    "                    \"tv_yayin\",\n",
    "                    \"siparis_nakil\",\n",
    "                    \"ekip\",\n",
    "                    \"bayi\",\n",
    "                    \"diger\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "converted_list = []\n",
    "for satir in open('C:\\\\Users\\\\IRPHAN\\\\Desktop\\\\New folder (6)\\\\stop_words_turkish.txt',encoding=\"utf-8\"):#https://countwordsfree.com/stopwords/turkish\n",
    "    converted_list.append(satir.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "TurkishMorphology = JClass('zemberek.morphology.TurkishMorphology')\n",
    "morphology = TurkishMorphology.createWithDefaults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to lowercase, strip and remove punctuations\n",
    "def preprocess(text):\n",
    "    text = text.lower() \n",
    "    text=text.strip()  \n",
    "    text=re.compile('<.*?>').sub('', text) \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)  \n",
    "    text = re.sub('\\s+', ' ', text)  \n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text) \n",
    "    text=re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n",
    "    text = re.sub(r'\\d',' ',text) \n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    return text\n",
    "\n",
    " \n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    a= [i for i in string.split() if i not in converted_list]\n",
    "    return ' '.join(a)\n",
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# Tokenize the sentence\n",
    "def lemmatizer(string):\n",
    "    analysis: java.util.ArrayList = (\n",
    "        morphology.analyzeAndDisambiguate(string).bestAnalysis()\n",
    "        )\n",
    "    pos: List[str] = []\n",
    "    for i, analysis in enumerate(analysis, start=1):\n",
    "        f'\\nAnalysis {i}: {analysis}',\n",
    "        f'\\nPrimary POS {i}: {analysis.getPos()}'\n",
    "        f'\\nPrimary POS (Short Form) {i}: {analysis.getPos().shortForm}'\n",
    "\n",
    "        pos.append(\n",
    "            f'{str(analysis.getLemmas()[0])}'\n",
    "            )\n",
    "    return \" \".join(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>metin</th>\n",
       "      <th>network_ici</th>\n",
       "      <th>network_disi</th>\n",
       "      <th>genisbant</th>\n",
       "      <th>gb_kopma</th>\n",
       "      <th>gb_altyapi</th>\n",
       "      <th>gb_port</th>\n",
       "      <th>gb_kablo</th>\n",
       "      <th>gb_hizveping</th>\n",
       "      <th>...</th>\n",
       "      <th>mb_hizveveri</th>\n",
       "      <th>mb_kopma</th>\n",
       "      <th>tivibu</th>\n",
       "      <th>tv_cihaz</th>\n",
       "      <th>tv_yayin</th>\n",
       "      <th>siparis_nakil</th>\n",
       "      <th>ekip</th>\n",
       "      <th>bayi</th>\n",
       "      <th>diger</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17237993</td>\n",
       "      <td>TTNET ev internetimin bina girişinden kablo ba...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>ttnet ev internet bina gir kablo bağlantı yer ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17237543</td>\n",
       "      <td>06/Ocak/2020 den beri sürekli arızalı olan ve ...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ocak de sürekli arıza teknik ekip ziyaret türl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17232767</td>\n",
       "      <td>100 lira fatura veriyorum, hala internetim yav...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>lira fatura ver hala internet yavaş internet o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17228655</td>\n",
       "      <td>Aylardır kullanıyoruz Türk Telekom ev internet...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>ay kullan türk telekom ev internet son ay git ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17227857</td>\n",
       "      <td>23.11.2020 tarihinde İstanbul Sancaktepe buluş...</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>tarih istanbul sancaktepe buluş sokak UNK site...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                              metin  network_ici  \\\n",
       "0  17237993  TTNET ev internetimin bina girişinden kablo ba...         True   \n",
       "1  17237543  06/Ocak/2020 den beri sürekli arızalı olan ve ...        False   \n",
       "2  17232767  100 lira fatura veriyorum, hala internetim yav...         True   \n",
       "3  17228655  Aylardır kullanıyoruz Türk Telekom ev internet...         True   \n",
       "4  17227857  23.11.2020 tarihinde İstanbul Sancaktepe buluş...         True   \n",
       "\n",
       "   network_disi  genisbant  gb_kopma  gb_altyapi  gb_port  gb_kablo  \\\n",
       "0          True       True      True        True     True      True   \n",
       "1         False      False     False       False    False     False   \n",
       "2         False       True     False       False    False     False   \n",
       "3         False       True      True       False    False     False   \n",
       "4         False       True     False       False     True     False   \n",
       "\n",
       "   gb_hizveping  ...  mb_hizveveri  mb_kopma  tivibu  tv_cihaz  tv_yayin  \\\n",
       "0          True  ...          True      True    True      True      True   \n",
       "1         False  ...         False     False   False     False     False   \n",
       "2          True  ...         False     False   False     False     False   \n",
       "3         False  ...         False     False   False     False     False   \n",
       "4         False  ...         False     False   False     False     False   \n",
       "\n",
       "   siparis_nakil   ekip   bayi  diger  \\\n",
       "0           True   True   True   True   \n",
       "1          False  False  False  False   \n",
       "2          False  False  False  False   \n",
       "3          False  False  False  False   \n",
       "4          False  False  False  False   \n",
       "\n",
       "                                          clean_text  \n",
       "0  ttnet ev internet bina gir kablo bağlantı yer ...  \n",
       "1  ocak de sürekli arıza teknik ekip ziyaret türl...  \n",
       "2  lira fatura ver hala internet yavaş internet o...  \n",
       "3  ay kullan türk telekom ev internet son ay git ...  \n",
       "4  tarih istanbul sancaktepe buluş sokak UNK site...  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pre-processing the dataset\n",
    "def finalpreprocess(string):\n",
    "    return lemmatizer(stopword(preprocess(string)))\n",
    "df_train['clean_text'] = df_train['metin'].apply(lambda x: finalpreprocess(x))\n",
    "df_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPLITTING THE TRAINING DATASET INTO TRAIN AND TEST\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_train[\"clean_text\"],df_train[\"network_ici\"],test_size=0.2,shuffle=True)\n",
    "#Word2Vec\n",
    "# Word2Vec runs on tokenized sentences\n",
    "X_train_tok= [nltk.word_tokenize(i) for i in X_train]  \n",
    "X_test_tok= [nltk.word_tokenize(i) for i in X_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass MeanEmbeddingVectorizer(object):\\n    def __init__(self, word2vec):\\n        self.word2vec = word2vec\\n        # if a text is empty we should return a vector of zeros\\n        # with the same dimensionality as all the other vectors\\n        self.dim = len(next(iter(word2vec.values())))\\n    def fit(self, X, y):\\n            return self\\n    def transform(self, X):\\n            return np.array([\\n                np.mean([self.word2vec[w] for w in words if w in self.word2vec]\\n                        or [np.zeros(self.dim)], axis=0)\\n                for words in X\\n            ])\\n  \\n\\nw2v = dict(zip(model.wv.index_to_key, model.wv.vectors))\\ndf_train['clean_text_tok']=[nltk.word_tokenize(i) for i in df_train['clean_text']]\\nmodel = Word2Vec(df_train['clean_text_tok'],min_count=1)     \\nmodelw = MeanEmbeddingVectorizer(w2v)\\n# converting text to numerical data using Word2Vec\\nX_train_vectors_w2v = modelw.transform(X_train_tok)\\nX_val_vectors_w2v = modelw.transform(X_test_tok)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tf-Idf\n",
    "tfidf_vectorizer = TfidfVectorizer(use_idf=True)\n",
    "X_train_vectors_tfidf = tfidf_vectorizer.fit_transform(X_train) \n",
    "X_test_vectors_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "#building Word2Vec model\n",
    "\"\"\"\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec):\n",
    "        self.word2vec = word2vec\n",
    "        # if a text is empty we should return a vector of zeros\n",
    "        # with the same dimensionality as all the other vectors\n",
    "        self.dim = len(next(iter(word2vec.values())))\n",
    "    def fit(self, X, y):\n",
    "            return self\n",
    "    def transform(self, X):\n",
    "            return np.array([\n",
    "                np.mean([self.word2vec[w] for w in words if w in self.word2vec]\n",
    "                        or [np.zeros(self.dim)], axis=0)\n",
    "                for words in X\n",
    "            ])\n",
    "  \n",
    "\n",
    "w2v = dict(zip(model.wv.index_to_key, model.wv.vectors))\n",
    "df_train['clean_text_tok']=[nltk.word_tokenize(i) for i in df_train['clean_text']]\n",
    "model = Word2Vec(df_train['clean_text_tok'],min_count=1)     \n",
    "modelw = MeanEmbeddingVectorizer(w2v)\n",
    "# converting text to numerical data using Word2Vec\n",
    "X_train_vectors_w2v = modelw.transform(X_train_tok)\n",
    "X_val_vectors_w2v = modelw.transform(X_test_tok)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1620)\t0.24142310507926085\n",
      "  (0, 2292)\t0.14238226002686805\n",
      "  (0, 734)\t0.20225681845057428\n",
      "  (0, 3293)\t0.1645767405970466\n",
      "  (0, 3161)\t0.16154358429175542\n",
      "  (0, 1987)\t0.19216731395177566\n",
      "  (0, 1476)\t0.10400698312193196\n",
      "  (0, 3373)\t0.1565599947163078\n",
      "  (0, 3402)\t0.1269246210506153\n",
      "  (0, 140)\t0.1479123278603232\n",
      "  (0, 2229)\t0.16593733688485932\n",
      "  (0, 1657)\t0.14919607052020742\n",
      "  (0, 208)\t0.16196540334121134\n",
      "  (0, 2490)\t0.2902813797721236\n",
      "  (0, 2228)\t0.31298160291271837\n",
      "  (0, 2604)\t0.18382564054170428\n",
      "  (0, 691)\t0.22332617054007642\n",
      "  (0, 1021)\t0.18586670371880953\n",
      "  (0, 674)\t0.09298776123896713\n",
      "  (0, 2089)\t0.3605879665688933\n",
      "  (0, 89)\t0.19888862550143838\n",
      "  (0, 1207)\t0.11181505228571564\n",
      "  (0, 1447)\t0.07101027833847182\n",
      "  (0, 2200)\t0.14919607052020742\n",
      "  (0, 1338)\t0.2874335216865927\n",
      "  (0, 654)\t0.16616787657871543\n",
      "  (0, 2829)\t0.07766714891189974\n",
      "  (0, 2935)\t0.07970661578717682\n"
     ]
    }
   ],
   "source": [
    "print(X_train_vectors_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.89      0.93      0.91       318\n",
      "        True       0.91      0.87      0.89       282\n",
      "\n",
      "    accuracy                           0.90       600\n",
      "   macro avg       0.90      0.90      0.90       600\n",
      "weighted avg       0.90      0.90      0.90       600\n",
      "\n",
      "Confusion Matrix: [[295  23]\n",
      " [ 37 245]]\n",
      "AUC: 0.9643605870020965\n"
     ]
    }
   ],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression(tf-idf)\n",
    "lr_tfidf=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "lr_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = lr_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.87      0.86      0.86       340\n",
      "        True       0.82      0.83      0.83       260\n",
      "\n",
      "    accuracy                           0.85       600\n",
      "   macro avg       0.84      0.85      0.84       600\n",
      "weighted avg       0.85      0.85      0.85       600\n",
      "\n",
      "Confusion Matrix: [[291  49]\n",
      " [ 43 217]]\n",
      "AUC: 0.9255769230769231\n"
     ]
    }
   ],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Logistic Regression (W2v)\n",
    "lr_w2v=LogisticRegression(solver = 'liblinear', C=10, penalty = 'l2')\n",
    "lr_w2v.fit(X_train_vectors_w2v, y_train)  #model\n",
    "#Predict y value for test dataset\n",
    "y_predict = lr_w2v.predict(X_val_vectors_w2v)\n",
    "y_prob = lr_w2v.predict_proba(X_val_vectors_w2v)[:,1]\n",
    "\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    " \n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2398, 100)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vectors_w2v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.87      0.90      0.88       340\n",
      "        True       0.86      0.83      0.84       260\n",
      "\n",
      "    accuracy                           0.87       600\n",
      "   macro avg       0.87      0.86      0.86       600\n",
      "weighted avg       0.87      0.87      0.87       600\n",
      "\n",
      "Confusion Matrix: [[305  35]\n",
      " [ 45 215]]\n",
      "AUC: 0.9354524886877829\n"
     ]
    }
   ],
   "source": [
    "#FITTING THE CLASSIFICATION MODEL using Naive Bayes(tf-idf)\n",
    "nb_tfidf = MultinomialNB()\n",
    "nb_tfidf.fit(X_train_vectors_tfidf, y_train)  \n",
    "#Predict y value for test dataset\n",
    "y_predict = nb_tfidf.predict(X_test_vectors_tfidf)\n",
    "y_prob = nb_tfidf.predict_proba(X_test_vectors_tfidf)[:,1]\n",
    "\n",
    "print(classification_report(y_test,y_predict))\n",
    "print('Confusion Matrix:',confusion_matrix(y_test, y_predict))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print('AUC:', roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre-processing the new dataset\n",
    "df_test['clean_text'] = df_test['metin'].apply(lambda x: finalpreprocess(x)) #preprocess the data\n",
    "X_test=df_test['clean_text'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                         clean_text  network_ici\n",
      "0  15500032  hat değiş önce telekom numara değiş prime müşt...        False\n",
      "1  14968518  no internet hat nakil al iste altyapı nakil ya...         True\n",
      "2  14495278  iş sebep sabit adres neredeyse yıl internet ku...        False\n",
      "3  12254763  dün türk telekom jet mobile internet fatura ya...        False\n",
      "4  17410879  fatura tarife yüksek gel yap öde sınır aşım du...        False\n"
     ]
    }
   ],
   "source": [
    "#converting words to numerical data using tf-idf\n",
    "X_vector=tfidf_vectorizer.transform(X_test)\n",
    "#use the best model to predict 'target' value for the new dataset \n",
    "y_predict = lr_tfidf.predict(X_vector)      \n",
    "y_prob = lr_tfidf.predict_proba(X_vector)[:,1]\n",
    "df_test['predict_prob']= y_prob\n",
    "df_test['network_ici']= y_predict\n",
    "final=df_test[['id','clean_text','network_ici']].reset_index(drop=True)\n",
    "print(final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = %88.08\n"
     ]
    }
   ],
   "source": [
    "#we compare the test results with exact values\r\n",
    "counter = 0\r\n",
    "i = 0\r\n",
    "j = 0\r\n",
    "for row in final[\"id\"]:\r\n",
    "    #print(row)\r\n",
    "    for row2 in df_comp[\"id\"]:\r\n",
    "        if(final[\"id\"][i] == df_comp[\"id\"][j]):\r\n",
    "            #print(i)\r\n",
    "            if(final[\"network_ici\"][i] == df_comp[\"network_ici\"][j]):\r\n",
    "                counter +=1      \r\n",
    "        j+=1\r\n",
    "    i += 1\r\n",
    "    j = 0\r\n",
    "    #print(\"counter = {}\".format(counter))\r\n",
    "print(\"Accuracy = %{}\".format(round((counter/i)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                         clean_text  network_ici\n",
      "0  15500032  hat değiş önce telekom numara değiş prime müşt...        False\n",
      "1  14968518  no internet hat nakil al iste altyapı nakil ya...        False\n",
      "2  14495278  iş sebep sabit adres neredeyse yıl internet ku...        False\n",
      "3  12254763  dün türk telekom jet mobile internet fatura ya...        False\n",
      "4  17410879  fatura tarife yüksek gel yap öde sınır aşım du...        False\n"
     ]
    }
   ],
   "source": [
    "#converting words to numerical data using word2vec\n",
    "X_vector=modelw.transform(X_test)\n",
    "#use the best model to predict 'target' value for the new dataset \n",
    "y_predict = lr_w2v.predict(X_vector)      \n",
    "y_prob = lr_w2v.predict_proba(X_vector)[:,1]\n",
    "df_test['predict_prob']= y_prob\n",
    "df_test['network_ici']= y_predict\n",
    "final=df_test[['id','clean_text','network_ici']].reset_index(drop=True)\n",
    "print(final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = %60.59\n"
     ]
    }
   ],
   "source": [
    "#we compare the test results with exact values\r\n",
    "counter = 0\r\n",
    "i = 0\r\n",
    "j = 0\r\n",
    "for row in final[\"id\"]:\r\n",
    "    #print(row)\r\n",
    "    for row2 in df_comp[\"id\"]:\r\n",
    "        if(final[\"id\"][i] == df_comp[\"id\"][j]):\r\n",
    "            #print(i)\r\n",
    "            if(final[\"network_ici\"][i] == df_comp[\"network_ici\"][j]):\r\n",
    "                counter +=1      \r\n",
    "        j+=1\r\n",
    "    i += 1\r\n",
    "    j = 0\r\n",
    "    #print(\"counter = {}\".format(counter))\r\n",
    "print(\"Accuracy = %{}\".format(round((counter/i)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                         clean_text  network_ici\n",
      "0  15500032  hat değiş önce telekom numara değiş prime müşt...        False\n",
      "1  14968518  no internet hat nakil al iste altyapı nakil ya...         True\n",
      "2  14495278  iş sebep sabit adres neredeyse yıl internet ku...        False\n",
      "3  12254763  dün türk telekom jet mobile internet fatura ya...        False\n",
      "4  17410879  fatura tarife yüksek gel yap öde sınır aşım du...        False\n"
     ]
    }
   ],
   "source": [
    "#converting words to numerical data using Naive Bayes(tf-idf)\n",
    "X_vector=tfidf_vectorizer.transform(X_test)\n",
    "#use the best model to predict 'target' value for the new dataset \n",
    "y_predict = nb_tfidf.predict(X_vector)      \n",
    "y_prob = nb_tfidf.predict_proba(X_vector)[:,1]\n",
    "df_test['predict_prob']= y_prob\n",
    "df_test['network_ici']= y_predict\n",
    "final=df_test[['id','clean_text','network_ici']].reset_index(drop=True)\n",
    "print(final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = %87.88\n"
     ]
    }
   ],
   "source": [
    "#we compare the test results with exact values\n",
    "counter = 0\n",
    "i = 0\n",
    "j = 0\n",
    "for row in final[\"id\"]:\n",
    "    #print(row)\n",
    "    for row2 in df_comp[\"id\"]:\n",
    "        if(final[\"id\"][i] == df_comp[\"id\"][j]):\n",
    "            #print(i)\n",
    "            if(final[\"network_ici\"][i] == df_comp[\"network_ici\"][j]):\n",
    "                counter +=1      \n",
    "        j+=1\n",
    "    i += 1\n",
    "    j = 0\n",
    "    #print(\"counter = {}\".format(counter))\n",
    "print(\"Accuracy = %{}\".format(round((counter/i)*100,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLP Tutorial for Text Classification in Python (with Twitter datas)\n",
    "#https://medium.com/analytics-vidhya/nlp-tutorial-for-text-classification-in-python-8f19cd17b49e "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}