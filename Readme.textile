<blockquote>
*TABLE OF CONTENTS*


<li><p>"Building Database 8":#building-database</p>
<ol style="list-style-type: decimal;">
<li><p>"Trained Data 9":#trained-data</p></li>
<li><p>"Test Data 9":#test-data</p></li>
<li><p>"Comparison Data 9":#comparison-data</p></li>
</ol>
</li>
<li>bq. <p>"Supervised Learning Algorithm 9":#supervised-learning-algorithm</p>


<ol style="list-style-type: decimal;">
<li><p>"Classification 9":#classification</p></li>
<li><p>"Text Pre&#45;Processing 10":#text-pre-processing</p></li>
<li><p>"Vectorization 13":#vectorization</p>
<ol style="list-style-type: decimal;">
<li><p>"Term Frequency&#45;Inverse Document Frequencies (tf&#45;idf) 13":#term-frequency-inverse-document-frequencies-tf-idf</p></li>
<li><p>"Word2Vec 13":#word2vec</p></li>
</ol>
</li>
<li><p>"ML Algorithm 14":#ml-algorithm</p>
<ol style="list-style-type: decimal;">
<li><p>"Logistic Regression 14":#logistic-regression</p>
<ol style="list-style-type: decimal;">
<li><p>"AUC (Area Under the Curve) 15":#auc-area-under-the-curve</p></li>
<li><p>"Precision 16":#precision</p></li>
<li>bq. <p>"Recall 16":#recall</p>

</li>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
</ol>
</li>
<li><p>"REFERENCES 18":#references</p></li>
</ol>




<ol start="2" style="list-style-type: decimal;">
<li>Building Database

<ol style="list-style-type: decimal;">
<li>PostgreSQL
</li>
</ol>
</li>
</ol>


_Figure 4__: Taking data from the database_

I take all data at once ["Figure 4":#_bookmark16].

!{width:626px;height:124px;}media/image7.jpeg(A picture containing calendar Description automatically generated)!

_Figure 5__: Saving data to the data frame_

</blockquote>

h5(#trained-data). Trained Data

bq. The data which you use to train an algorithm or machine learning model to predict the outcome. Out of the 158000 data given to me by the company, out of the 4000 data I have categorized as I mentioned above, I have reserved 3000 data for the training data and kept it in a data frame ["Figure 5":#_bookmark18].



h5(#test-data). Test Data

bq. Our test data, also known as raw data, was set to 1000 data from 4000 data.



h5(#comparison-data). Comparison Data

bq. Our comparison data is a data set whose categories we already know. Using this data, we can tell if the predictions are correct by comparing the result of the projections of our test data.



h2(#supervised-learning-algorithm). Supervised Learning Algorithm

<blockquote>

Supervised learning uses a training dataset to teach models to yield the desired output. This training dataset includes inputs and correct outputs, which allow the model to learn over time. The algorithm measures its accuracy through the loss function, adjusting until the error has been sufficiently minimized (IBM, 2020).

Supervised learning can be separated into two types of problems when data mining&#45; classification and regression. I used classification in my project.

</blockquote>

h3(#classification). Classification

bq. Classification uses an algorithm to assign test data to specific categories ["Figure 6":#_bookmark25] correctly.



!{width:252px;height:223px;}media/image8.png!

<blockquote>

_Figure 6__: Classification graph_

As you see in figure "[9":#_bookmark25], we also separate our train data into two categories, network&#95;içi, and network&#95;dışı. network&#95;içi problems generally include connection errors, cable problems, and device problems. On the other hand, network&#95;dışı problems typically have technical issues and order, internet transfer problems. We took it as one if the complaint is related to the network&#95;içi, and 0 if not ["Figure 7":#_bookmark27].

</blockquote>

!{width:356px;height:52px;}media/image9.png!!{width:336px;height:208px;}media/image10.png!

bq. _Figure 7__: Separated data_



h3(#text-pre-processing). Text Pre&#45;Processing

<blockquote>

We must first preprocess our dataset by eliminating punctuation and special characters, cleaning texts, deleting stop words, and applying lemmatization before going on to model construction. Python provides us with some libraries to do these operations on English texts such as;

from nltk.tokenize import word&#95;tokenize from nltk.corpus import stopwords

from nltk.tokenize import word&#95;tokenize from nltk.stem import SnowballStemmer from nltk.corpus import wordnet

from nltk.stem import WordNetLemmatizer.

In order to use these operations on Turkish texts, I had to use external libraries such as Zemberek. Zemberek is a natural language processing library that you can use for open&#45;source Turkish languages developed using Java programming language ["Figure 8":#_bookmark30].

</blockquote>

!{width:565px;height:57px;}media/image11.png!

<blockquote>

_Figure 8__: Implementing Zemberek library_

After that, I used this library to preprocess. Also, I have downloaded a stop word txt file for the Turkish language ["Figure 9":#_bookmark32].

</blockquote>

!{width:624px;height:68px;}media/image12.jpeg!

bq. _Figure 9__: Implementing stop word data_



!{width:619px;height:53px;}media/image13.jpeg!!{width:411px;height:435px;}media/image14.jpeg!

<blockquote>

_Figure 10__: Preprocessing functions_

!{width:600px;height:94px;}media/image15.png(Text Description automatically generated)!

I cleaned the text by using preprocessing functions. Firstly, I removed all capital words and punctuations. And then, I applied a stopword list to our text and removed all stop words in our text. Finally, I used the Zemberek library to lemmatize our text ["Figure 10":#_bookmark34].

After preprocessing, finally, we get our clean text ["Figure 11":#_bookmark36].

</blockquote>

!{width:450px;height:437px;}media/image16.png!

bq. _Figure 11__: Cleaned text_



h3(#vectorization). Vectorization

h4(#term-frequency-inverse-document-frequencies-tf-idf). Term Frequency&#45;Inverse Document Frequencies (tf&#45;idf)

bq. Traditional TF&#45;IDF (Term Frequency&#45;Inverse Document Frequency) feature weighting algorithm only uses word frequency information to measure the importance of feature items in the data set (Wu &amp; Yuan, 2018). It is directly proportional to the word's frequency of occurrence in the text and inversely proportional to the frequency of occurrence in the sentence.



h4(#word2vec). Word2Vec

<blockquote>

Word2Vec, proposed and supported by Google, is not a unique algorithm, but it consists of two learning models, Continuous Bag of Words (CBOW) and Skip&#45;gram (Ma &amp; Zhang, 2015). I also used word2vec in my project, but my word2vec model did not work very effectively.

We can convert our text input to numerical form using any of these methods, which will be utilized to develop the categorization model. As I mentioned above, I allocated %75 of the dataset for the trained data and %25 for the test data by using the code below ["Figure 12":#_bookmark41].

</blockquote>

!{width:624px;height:81px;}media/image17.jpeg!

<blockquote>

_Figure 12__: Splitting datasets_

The code for vectorization by using Tf&#45;Idf "[Figure 13":#_bookmark43].

</blockquote>

!{width:556px;height:112px;}media/image18.png!

<blockquote>

_Figure 13__: Tf&#45;Idf code_

As you see in "Figure 14":#_bookmark45,the left part coordinates of non&#45;zero values and in the right part, values at that point.

</blockquote>

!{width:239px;height:259px;}media/image19.png!

bq. _Figure_ _14:Train vectors_



h3(#ml-algorithm). ML Algorithm

h6(#logistic-regression). Logistic Regression

<blockquote>

Logistic regression permits the use of continuous or categorical predictors and provides the ability to adjust for multiple predictors. This makes logistic regression especially useful for analyzing observational data when adjustment is needed to reduce the potential bias resulting from differences in the groups being compared (LaValley, 2008).

I imported the logistic regression using Python's sklearn library ["Figure 15":#_bookmark49].

</blockquote>

!{width:622px;height:140px;}media/image20.jpeg!

<blockquote>

_Figure_ _15:Sklearn Library_

I implemented classification model using Logistic Regression ["Figure 16":#_bookmark51].

</blockquote>

!{width:463px;height:365px;}media/image21.png!

bq. _Figure_ _16:Tf&#45;Idf Model_



h6(#auc-area-under-the-curve). AUC (Area Under the Curve)

<blockquote>

The Area Under the ROC curve (AUC) is an aggregated metric that evaluates how well a logistic regression model classifies positive and negative outcomes at all possible cutoffs. Our AUC value is 0.96, very close to 1 it is considered an outstanding score ["Figure 17":#_bookmark54].

Except for AUC, all the measures may be calculated using the four parameters on the left "[1515":#_bookmark54].

</blockquote>

!{width:571px;height:143px;}media/image22.jpeg!

<blockquote>

_Figure 17__: Parameters_

True Positives (TP): These are correctly predicted positive values. True Negatives (TN): These are correctly predicted negative values.

False Positives (FP): When the predicted and the actual value do not match False Negatives (FN): When the expected and the actual value do not match

</blockquote>

h6(#precision). Precision

<blockquote>

The ratio of accurately predicted positive observations to total expected positive observations is known as precision.

Precision = TP/TP&#43;FP

</blockquote>

h6(#recall). Recall

<blockquote>

The ratio of accurately predicted positive observations to all observations in the actual class is known as recall.

Recall = TP/TP&#43;FN

</blockquote>

h6(#f1-score). F1&#45;Score

<blockquote>

The weighted average of Precision and Recall is the F1 Score. As a result, this score considers both false positives and false negatives.

F1 Score = 2&#42;(Recall &#42; Precision) / (Recall &#43; Precision)

Finally, we implement our Tf&#45;If model to our test data. However, we need to clean our text data before the implementation process ["Figure 18":#_bookmark59].

</blockquote>

!{width:622px;height:296px;}media/image23.jpeg!

<blockquote>

_Figure 18__: Implementing the model_

To check if our predictions were correct, I compared my final data with the comparison data whose values I already know ["Figure 19":#_bookmark61].

</blockquote>

!{width:532px;height:259px;}media/image24.png!

bq. _Figure 19__: Accuracy_




h1(#references). REFERENCES

# "[15":#_bookmark54]Accuracy, Precision, Recall &amp; F1 Score: Interpretation of Performance Measures. "https://blog.exsilio.com/all/accuracy&#45;precision&#45;recall&#45;f1&#45;score&#45;interpretation&#45;of&#45; performance&#45;measures/":https://blog.exsilio.com/all/accuracy-precision-recall-f1-score-interpretation-of-performance-measures/
# IBM. (2020, August 19). _Supervised Learning_

bq. https:/"/www.ibm.com/cloud/learn/supervised&#45;learning":http://www.ibm.com/cloud/learn/supervised-learning



#3 "[9":#_bookmark25] "https://www.javatpoint.com/classification&#45;algorithm&#45;in&#45;machine&#45;learning":https://www.javatpoint.com/classification-algorithm-in-machine-learning
# LaValley, M. P. (2008). Logistic Regression. " https://www.ahajournals.org/doi/full/10.1161/CIRCULATIONAHA.106.682658":https://www.ahajournals.org/doi/full/10.1161/CIRCULATIONAHA.106.682658
# Ma, L. &amp; Zhang, Y. (2015). _Using Word2Vec to process big text data._

bq. https://ieeexplore.ieee.org/abstract/document/7364114



#6 "[7":#_bookmark10] "https://www.sikayetvar.com/turk&#45;telekom":https://www.sikayetvar.com/turk-telekom
# The official website of Postgresql https://"www.postgresql.org":http://www.postgresql.org/
# Wu, H., &amp; Yuan, N. (2018). _An Improved TF&#45;IDF algorithm based on word frequency distribution information and category distribution information._

<blockquote>

9. "https://dl.acm.org/doi/abs/10.1145/3232116.3232152":https://dl.acm.org/doi/abs/10.1145/3232116.3232152

"10. [11":#_bookmark30] Zemberek Kütüphanesi ile Türkçe Metinlerde Kelime Köklerinin Bulunması. "https://melikebektas95.medium.com/zemberek&#45;kütüphanesi&#45;ile&#45;türkçe&#45;metinlerde&#45; kelime&#45;köklerinin&#45;bulunması&#45;6ddd3a875d5f":https://melikebektas95.medium.com/zemberek-kÃ¼tÃ¼phanesi-ile-tÃ¼rkÃ§e-metinlerde-kelime-kÃ¶klerinin-bulunmasÄ±-6ddd3a875d5f

</blockquote>


